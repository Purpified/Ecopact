import os
import pdb
# from readline import replace_history_item
from bardapi import Bard
from flask import Flask, request, jsonify
from flask_cors import CORS
import joblib
import numpy as np
from keras.models import load_model
from pyparsing import replace_with
from xgboost import XGBRegressor
from pandas import DataFrame

app = Flask(__name__)
CORS(app)

os.environ['_BARD_API_KEY'] = 'XQioAemGhqCVyyCpevEoko2VYuzou8Xg5W2b_STl-gHHbQDmw1_11LhQwF2eCyHw9u9QdA.'

def map_prediction_to_string(prediction):
    reverse_replace_dict = {
        0: 'Very Low',
        1: 'Low',
        2: 'Medium-Low',
        3: 'Medium',
        4: 'Medium-High',
        5: 'High',
        6: 'Very High'
    }
    return reverse_replace_dict[prediction]


@app.route("/product", methods=["POST"])
def product():
    data = request.get_json()
    value = data.get('value', '')
        # -*- coding: utf-8 -*-
    """Copy of NLPEnviornment

    Automatically generated by Colaboratory.

    Original file is located at
        https://colab.research.google.com/drive/16CQZO4TsWyqJ_7ySQX1fZC--sXvRtbRu
    """

    import pandas as pd
    from sklearn.decomposition import DictionaryLearning
    import pandas as pd
    print(pd.__version__)
    path = "dataset.csv"
    dataset = pd.read_csv(path)
    dataset.head(3)

    dataset.info()


    import json
    import ast
    from bardapi import Bard
    import os

    os.environ['_BARD_API_KEY'] = 'XwhWFK57yz8L6Ai8-e3YH5gQSPQ5LG-kGvxzfBjoZSf_z5xTC-bbMe0CQZpJe7C-rL00mw.'

    # Function to extract python dictionary string from the answer
    def extract_dict_string(answer):
        index1 = answer.find("{")
        index2 = answer.rfind("}")  # Changed to rfind to find the last occurrence of }
        answer = answer[index1:index2+1]
        return answer

    dOne = 0
    while dOne == 0:
        answer1 = Bard().get_answer("DONT SAY SURE OR CONFIRM ANYTHING. STOP EXPLAINING STUFF.  STOP EXPLAINING ONLY GIVE THE DEFINITION. ONLY output all the raw materials used in a " + value + " in a python dictionary expression format, with the second part being all the raw materials listed together and the first part being called 'Materials'. Next, give one output for recyclability and biodegradability on a scale of no, yes and partial. Keep it all in python dictionary format")['content']
        answer1 = extract_dict_string(answer1)
        answer1 = answer1.replace(",}", "}")
        print(answer1)

        try:
            dictionary1 = ast.literal_eval(answer1)
            dOne = 1
        except SyntaxError:
            print(f"Could not parse answer1 into a dictionary. Here's what answer1 looks like: {answer1}")

    dTwo = 0
    while dTwo == 0:
        answer2 = Bard().get_answer("DONT SAY SURE OR CONFIRM ANYTHING. STOP EXPLAINING STUFF. ONLY output energy_consumption, electricity_usage, gasoline_usage, water_usage, emission_levels, , recycled_materials_percentage, and toxicity on a scale of low, medium and high in python dictionary format for a " + value)['content']
        answer2 = extract_dict_string(answer2)
        answer2 = answer2.replace(",}", "}")
        print(answer2)

        try:
            dictionary2 = ast.literal_eval(answer2)
            dTwo = 1
        except SyntaxError:
            print(f"Could not parse answer2 into a dictionary. Here's what answer2 looks like: {answer1}")

    complete_info = {**dictionary1, **dictionary2}

    print(complete_info)

    with open("sample.json", "w") as outfile:
        json.dump(complete_info, outfile)

    from sklearn.preprocessing import OneHotEncoder

    # get the list of materials
    #materials = list(DictionaryLearning.keys())

    # one hot encoding
    encoder = OneHotEncoder(sparse=False)
    #encoded_materials = encoder.fit_transform([materials])

    #print(encoded_materials)

    # create a new dataframe
    df = pd.DataFrame(columns=['Energy Consumption', 'Electricity Usage', 'Gasoline Usage', 'Water Usage', 'Emission Levels', 'Recyclability', 'Biodegradability', '% of Recycled Materials Used', 'Toxicity', 'Materials'])

    # fill in the values
    df = df.append(complete_info, ignore_index=True)

    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.metrics import mean_squared_error

    replace_dict = {
        'None': 0,
        'Yes': 1,
        'No': 0,
        'Partial': 0.5,
        'Very Low': 0,
        'Low': 1,
        'Medium-Low': 2,
        'Medium': 3,
        'Medium-High': 4,
        'High': 5,
        'Very High': 6
    }

    for column in dataset.columns:
        if dataset[column].dtype == 'object':
            dataset[column] = dataset[column].replace(replace_dict)

    if 'Location' in dataset.columns:
        dataset = dataset.drop(['Location'], axis=1)

    if 'Item' in dataset.columns:
        dataset = dataset.drop(['Item'], axis=1)


    dataset_encoded = pd.get_dummies(dataset, columns=['Materials'])

    features = dataset_encoded.drop('Overall environmental ranking', axis=1)
    labels = dataset_encoded['Overall environmental ranking']

    features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.2)

    model = RandomForestRegressor()
    model.fit(features_train, labels_train)

    predictions = model.predict(features_test)

    mse = mean_squared_error(labels_test, predictions)

    print(f"Mean Squared Error: {mse}")

    from keras.models import Sequential
    from keras.layers import Dense, Dropout
    from keras.callbacks import EarlyStopping
    from keras.layers import BatchNormalization
    from keras.optimizers import Adam
    from keras.regularizers import l2
    from numpy.random import seed

    # Initialize the model
    model_nn = Sequential()
    seed(1)

    # Add layers
    model_nn.add(Dense(128, input_dim=features_train.shape[1], activation='relu', kernel_regularizer=l2(0.001)))  # Input layer
    model_nn.add(BatchNormalization())
    model_nn.add(Dropout(0.1))  # Dropout layer
    model_nn.add(Dense(64, activation='relu', kernel_regularizer=l2(0.001)))  # Hidden layer
    model_nn.add(BatchNormalization())
    model_nn.add(Dropout(0.1))  # Another dropout layer
    model_nn.add(Dense(32, activation='relu', kernel_regularizer=l2(0.001)))  # Another hidden layer
    model_nn.add(BatchNormalization())
    model_nn.add(Dropout(0.1))  # Another dropout layer
    model_nn.add(Dense(1))  # Output layer

    # Compile the model
    opt = Adam(learning_rate=0.001)
    model_nn.compile(loss='mean_squared_error', optimizer=opt)

    # Define Early stopping
    early_stopping = EarlyStopping(monitor='val_loss', patience=5)

    # Train the model
    history = model_nn.fit(features_train, labels_train, epochs=200, batch_size=16, verbose=1, callbacks=[early_stopping], validation_split=0.2)

    # Predict on the testing set
    y_pred_nn = model_nn.predict(features_test)
    print(y_pred_nn)

    # Evaluate the model using Mean Squared Error
    mse_nn = mean_squared_error(labels_test, y_pred_nn)
    print(f"Neural Network Mean Squared Error: {mse_nn}")

    from xgboost import XGBRegressor
    from sklearn.model_selection import GridSearchCV
    from xgboost import XGBClassifier

    param_grid = {
        'n_estimators': [50, 100, 150, 200],
        'learning_rate': [0.01, 0.1, 0.2, 0.3],
        'max_depth': [3,4,5,6],
        'gamma':[i/10.0 for i in range(0,5)],
        # 'subsample':[i/10.0 for i in range(6,10)],
        # 'colsample_bytree':[i/10.0 for i in range(6,10)],
        # 'min_child_weight':[6,8,10,12],
        # 'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]
    }

    # Initialize the model
    xgb_model = XGBRegressor(random_state=0)

    # Create the grid search object
    grid_search = GridSearchCV(estimator=xgb_model, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)

    # Fit the grid search object to the data
    grid_search.fit(features_train, labels_train)

    # Retrain the model with the best parameters
    xgb_model = XGBRegressor(**grid_search.best_params_, random_state=0)
    xgb_model.fit(features_train, labels_train.ravel())

    # Predict on the testing set
    labels_pred_xgb = xgb_model.predict(features_test)

    # Evaluate the model using Mean Squared Error
    mse_xgb = mean_squared_error(labels_test, labels_pred_xgb)
    print(f"XGBoost Mean Squared Error: {mse_xgb}")

    # Create a dataframe from bards output
    df_bard = pd.DataFrame([complete_info])

    replace_dict = {
        'None': 0,
        'Yes': 1,
        'No': 0,
        'Partial': 0.5,
        'Very Low': 0,
        'Low': 1,
        'Medium-Low': 2,
        'Medium': 3,
        'Medium-High': 4,
        'High': 5,
        'Very High': 6
    }

    # Replace values and ensure column names match those in the training data
    df_bard.columns = df_bard.columns.str.lower().str.replace(' ', '_')
    for column in df_bard.columns:
        if df_bard[column].dtype == 'object' and column != 'materials':
            df_bard[column] = df_bard[column].replace(replace_dict)

    # Make sure the 'Materials' column contains lists, not strings
    df_bard['materials'] = df_bard['materials'].apply(lambda x: [x] if isinstance(x, str) else x)

    # Generate dummy variables from 'Materials' column (one-hot encoding)
    materials_dummies = pd.get_dummies(df_bard.materials.apply(pd.Series).stack()).sum(level=0)

    # Merge this back to the original dataframe
    df_bard = pd.concat([df_bard.drop('materials', axis=1), materials_dummies], axis=1)

    # For the columns which were in the training dataset, but are not present in the df_bard,
    # we need to add those columns to df_bard with a value of 0
    missing_cols = set(features.columns) - set(df_bard.columns)
    for c in missing_cols:
        df_bard[c] = 0

    # Ensure the order of column in the test dataset is in the same order than in train dataset
    df_bard = df_bard[features.columns]

    # Now you can predict
    prediction = xgb_model.predict(df_bard)

    import numpy as np

    rounded_predictions = np.around(prediction)
    return jsonify({"value": str(rounded_predictions)})


if __name__ == "__main__":
    app.run(debug=True)